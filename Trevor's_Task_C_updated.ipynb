{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 1. IMPORTS AND SETUP\n",
    "# =========================================================\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from functools import partial\n",
    "import random\n",
    "\n",
    "# =========================================================\n",
    "# 2. CONFIGURATION\n",
    "# =========================================================\n",
    "DATA_PATH     = \"expanded_lexicon.xlsx\"\n",
    "LANGUAGE_COLS = [\"Zulu\", \"Sepedi\", \"Xhosa\"]   # üëà African languages for training/eval\n",
    "ENGLISH_COL   = \"English\"                     # for translation display\n",
    "LABEL_COL     = \"Sentiment\"\n",
    "MODEL_NAME    = \"castorini/afriberta_base\"\n",
    "MAX_LEN       = 128\n",
    "BATCH_SIZE    = 8\n",
    "NUM_EPOCHS    = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "SEED          = 42\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "random.seed(SEED)\n",
    "\n",
    "# =========================================================\n",
    "# 3. DATA PREPARATION (train only on African languages)\n",
    "# =========================================================\n",
    "def load_african_data():\n",
    "    df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "    for col in LANGUAGE_COLS + [ENGLISH_COL, LABEL_COL]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"label\"] = label_encoder.fit_transform(df[LABEL_COL])\n",
    "\n",
    "    # Combine all African language data into one DataFrame\n",
    "    dfs = []\n",
    "    for lang in LANGUAGE_COLS:\n",
    "        temp = df[[lang, LABEL_COL, \"label\"]].dropna().copy()\n",
    "        temp = temp.rename(columns={lang: \"text\"})\n",
    "        temp[\"language\"] = lang\n",
    "        dfs.append(temp)\n",
    "\n",
    "    full_lang_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Split train/val\n",
    "    train_df, val_df = train_test_split(\n",
    "        full_lang_df,\n",
    "        test_size=0.2,\n",
    "        random_state=SEED,\n",
    "        stratify=full_lang_df[\"label\"]\n",
    "    )\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    val_ds   = Dataset.from_pandas(val_df)\n",
    "    return train_ds, val_ds, label_encoder, df\n",
    "\n",
    "# =========================================================\n",
    "# 4. TOKENIZATION\n",
    "# =========================================================\n",
    "def tokenize_function(batch, tokenizer, max_len):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "\n",
    "# =========================================================\n",
    "# 5. METRICS\n",
    "# =========================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# 6. SENTENCE GENERATION AND EVALUATION\n",
    "# =========================================================\n",
    "def generate_sentences_and_evaluate(trainer, df, label_encoder, tokenizer):\n",
    "    print(\"\\nüß© Generating and evaluating lexicon-based sentences...\\n\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for lang in LANGUAGE_COLS:\n",
    "        if lang not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è Skipping {lang} - column not found\")\n",
    "            continue\n",
    "\n",
    "        # Create short synthetic sentences using translated words\n",
    "        lang_words = df[lang].dropna().tolist()\n",
    "        eng_words  = df[ENGLISH_COL].dropna().tolist()\n",
    "\n",
    "        n = min(len(lang_words), len(eng_words))\n",
    "        lang_words = lang_words[:n]\n",
    "        eng_words = eng_words[:n]\n",
    "\n",
    "        # Randomly create 25 short sentences\n",
    "        num_sentences = 25\n",
    "        for _ in range(num_sentences):\n",
    "            idxs = random.sample(range(n), k=min(5, n))\n",
    "            afr_sentence = \" \".join([lang_words[i] for i in idxs])\n",
    "            eng_sentence = \" \".join([eng_words[i] for i in idxs])\n",
    "\n",
    "            # Tokenize and predict\n",
    "            inputs = tokenizer(\n",
    "                afr_sentence,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=MAX_LEN\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = trainer.model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "                pred_label = torch.argmax(probs, dim=1).item()\n",
    "                pred_sentiment = label_encoder.inverse_transform([pred_label])[0]\n",
    "\n",
    "            results.append({\n",
    "                \"Language\": lang,\n",
    "                \"African_Sentence\": afr_sentence,\n",
    "                \"English_Translation\": eng_sentence,\n",
    "                \"Predicted_Sentiment\": pred_sentiment\n",
    "            })\n",
    "\n",
    "    # Save to CSV\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_path = \"african_sentence_evaluation_results.csv\"\n",
    "    out_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ Results saved to: {out_path}\\n\")\n",
    "\n",
    "    # Show sample outputs\n",
    "    print(out_df.head(10).to_string(index=False))\n",
    "\n",
    "# =========================================================\n",
    "# 7. MAIN FUNCTION\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"üöÄ Loading African language data...\")\n",
    "    train_ds, val_ds, label_encoder, full_df = load_african_data()\n",
    "\n",
    "    print(\"üî§ Loading tokenizer and model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "    num_proc = min(4, os.cpu_count() or 1)\n",
    "    tokenize_partial = partial(tokenize_function, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "    print(\"üìù Tokenizing datasets...\")\n",
    "    train_ds = train_ds.map(tokenize_partial, batched=True, num_proc=num_proc)\n",
    "    val_ds   = val_ds.map(tokenize_partial, batched=True, num_proc=num_proc)\n",
    "\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    print(\"üß† Initializing model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_african\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        logging_steps=100,\n",
    "        warmup_ratio=0.06,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        save_total_limit=2,\n",
    "        dataloader_num_workers=num_proc,\n",
    "        report_to=\"none\",\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\"üèãÔ∏è Training AfriBERTa on African languages...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"‚úÖ Evaluating best model on validation set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"\\n=== African Languages Validation Results ===\")\n",
    "    print(eval_results)\n",
    "\n",
    "    trainer.save_model(\"./final_afriberta_african_model\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Generate and evaluate synthetic sentences\n",
    "    # ---------------------------------------------------------\n",
    "    generate_sentences_and_evaluate(trainer, full_df, label_encoder, tokenizer)\n",
    "\n",
    "# =========================================================\n",
    "# 8. ENTRY POINT\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
