{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9183d30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading African language data...\n",
      "üî§ Loading tokenizer and model...\n",
      "üìù Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10321/10321 [00:39<00:00, 261.05 examples/s]\n",
      "Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2581/2581 [00:10<00:00, 236.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at castorini/afriberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 230\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# 8. ENTRY POINT\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 179\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müß† Initializing model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    174\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m    175\u001b[39m     MODEL_NAME,\n\u001b[32m    176\u001b[39m     num_labels=\u001b[38;5;28mlen\u001b[39m(label_encoder.classes_)\n\u001b[32m    177\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results_african\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.06\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\u001b[32m    201\u001b[39m trainer = Trainer(\n\u001b[32m    202\u001b[39m     model=model,\n\u001b[32m    203\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m    209\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:135\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\116su\\source\\repos\\Assignment_3_Code\\venv\\Lib\\site-packages\\transformers\\training_args.py:1811\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1809\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.torchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1814\u001b[39m     warnings.warn(\n\u001b[32m   1815\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`torchdynamo` is deprecated and will be removed in version 5 of ü§ó Transformers. Use\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1816\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_compile_backend` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1817\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1818\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\116su\\source\\repos\\Assignment_3_Code\\venv\\Lib\\site-packages\\transformers\\training_args.py:2355\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2351\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2352\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2353\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2354\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\functools.py:1042\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m   1040\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m-> \u001b[39m\u001b[32m1042\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1044\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\116su\\source\\repos\\Assignment_3_Code\\venv\\Lib\\site-packages\\transformers\\training_args.py:2225\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2225\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2226\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2227\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2228\u001b[39m         )\n\u001b[32m   2229\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2230\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. IMPORTS AND SETUP\n",
    "# =========================================================\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from functools import partial\n",
    "import random\n",
    "\n",
    "# =========================================================\n",
    "# 2. CONFIGURATION\n",
    "# =========================================================\n",
    "DATA_PATH     = \"./lexicon_dataset/expanded_lexicon_cleaned.xlsx\"\n",
    "LANGUAGE_COLS = [\"Zulu\", \"Xhosa\"]   # üëà African languages for training/eval\n",
    "ENGLISH_COL   = \"English\"                     # for translation display\n",
    "LABEL_COL     = \"Sentiment\"\n",
    "MODEL_NAME    = \"castorini/afriberta_base\"\n",
    "MAX_LEN       = 128\n",
    "BATCH_SIZE    = 16\n",
    "NUM_EPOCHS    = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "SEED          = 42\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "random.seed(SEED)\n",
    "\n",
    "# =========================================================\n",
    "# 3. DATA PREPARATION (train only on African languages)\n",
    "# =========================================================\n",
    "def load_african_data():\n",
    "    df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "    for col in LANGUAGE_COLS + [ENGLISH_COL, LABEL_COL]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"label\"] = label_encoder.fit_transform(df[LABEL_COL])\n",
    "\n",
    "    # Combine all African language data into one DataFrame\n",
    "    dfs = []\n",
    "    for lang in LANGUAGE_COLS:\n",
    "        temp = df[[lang, LABEL_COL, \"label\"]].dropna().copy()\n",
    "        temp = temp.rename(columns={lang: \"text\"})\n",
    "        temp[\"language\"] = lang\n",
    "        dfs.append(temp)\n",
    "\n",
    "    full_lang_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Split train/val\n",
    "    train_df, val_df = train_test_split(\n",
    "        full_lang_df,\n",
    "        test_size=0.2,\n",
    "        random_state=SEED,\n",
    "        stratify=full_lang_df[\"label\"]\n",
    "    )\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    val_ds   = Dataset.from_pandas(val_df)\n",
    "    return train_ds, val_ds, label_encoder, df\n",
    "\n",
    "# =========================================================\n",
    "# 4. TOKENIZATION\n",
    "# =========================================================\n",
    "def tokenize_function(batch, tokenizer, max_len):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "\n",
    "# =========================================================\n",
    "# 5. METRICS\n",
    "# =========================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# 6. SENTENCE GENERATION AND EVALUATION\n",
    "# =========================================================\n",
    "def generate_sentences_and_evaluate(trainer, df, label_encoder, tokenizer):\n",
    "    print(\"\\nüß© Generating and evaluating lexicon-based sentences...\\n\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for lang in LANGUAGE_COLS:\n",
    "        if lang not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è Skipping {lang} - column not found\")\n",
    "            continue\n",
    "\n",
    "        # Create short synthetic sentences using translated words\n",
    "        lang_words = df[lang].dropna().tolist()\n",
    "        eng_words  = df[ENGLISH_COL].dropna().tolist()\n",
    "\n",
    "        n = min(len(lang_words), len(eng_words))\n",
    "        lang_words = lang_words[:n]\n",
    "        eng_words = eng_words[:n]\n",
    "\n",
    "        # Randomly create 25 short sentences\n",
    "        num_sentences = 25\n",
    "        for _ in range(num_sentences):\n",
    "            idxs = random.sample(range(n), k=min(5, n))\n",
    "            afr_sentence = \" \".join([lang_words[i] for i in idxs])\n",
    "            eng_sentence = \" \".join([eng_words[i] for i in idxs])\n",
    "\n",
    "            # Tokenize and predict\n",
    "            inputs = tokenizer(\n",
    "                afr_sentence,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=MAX_LEN\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = trainer.model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "                pred_label = torch.argmax(probs, dim=1).item()\n",
    "                pred_sentiment = label_encoder.inverse_transform([pred_label])[0]\n",
    "\n",
    "            results.append({\n",
    "                \"Language\": lang,\n",
    "                \"African_Sentence\": afr_sentence,\n",
    "                \"English_Translation\": eng_sentence,\n",
    "                \"Predicted_Sentiment\": pred_sentiment\n",
    "            })\n",
    "\n",
    "    # Save to CSV\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_path = \"african_sentence_evaluation_results.csv\"\n",
    "    out_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ Results saved to: {out_path}\\n\")\n",
    "\n",
    "    # Show sample outputs\n",
    "    print(out_df.head(10).to_string(index=False))\n",
    "\n",
    "# =========================================================\n",
    "# 7. MAIN FUNCTION\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"üöÄ Loading African language data...\")\n",
    "    train_ds, val_ds, label_encoder, full_df = load_african_data()\n",
    "\n",
    "    print(\"üî§ Loading tokenizer and model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "    num_proc = min(4, os.cpu_count() or 1)\n",
    "    tokenize_partial = partial(tokenize_function, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "    print(\"üìù Tokenizing datasets...\")\n",
    "    train_ds = train_ds.map(tokenize_partial, batched=True, num_proc=num_proc)\n",
    "    val_ds   = val_ds.map(tokenize_partial, batched=True, num_proc=num_proc)\n",
    "\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    print(\"üß† Initializing model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_african\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        logging_steps=100,\n",
    "        warmup_ratio=0.06,\n",
    "        save_total_limit=2,\n",
    "        dataloader_num_workers=num_proc,\n",
    "        report_to=\"none\",\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\"üèãÔ∏è Training AfriBERTa on African languages...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"‚úÖ Evaluating best model on validation set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"\\n=== African Languages Validation Results ===\")\n",
    "    print(eval_results)\n",
    "\n",
    "    trainer.save_model(\"./final_afriberta_african_model\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Generate and evaluate synthetic sentences\n",
    "    # ---------------------------------------------------------\n",
    "    generate_sentences_and_evaluate(trainer, full_df, label_encoder, tokenizer)\n",
    "\n",
    "# =========================================================\n",
    "# 8. ENTRY POINT\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
